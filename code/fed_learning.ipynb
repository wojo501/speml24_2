{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Pytorch\n",
    "First, import the Pytorch dependencies.\n",
    "\n",
    "Then we define the remote workers Alice and Bob, who will host the remote data while a local worker (or client) will manage the learning task. Note that we use virtual workers: these workers act just like normal remote workers except that they exist within the same Python program. Thus, we still serialize the commands to be exchanged between the workers but we donâ€™t actually send them over the network. This approach allows us to avoid network-related issues and focus on evaluation of the approach on a local setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import syft\n",
    "import wandb\n",
    "from typing import Dict\n",
    "import dotenv\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 2\n",
      "Python-dotenv could not parse statement starting at line 4\n"
     ]
    }
   ],
   "source": [
    "dotenv.load_dotenv() \n",
    "WANDB_KEY = os.getenv(\"WANDB_KEY\")\n",
    "ENTITY = os.getenv(\"ENTITY\")\n",
    "NUM_WORKERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = syft.TorchHook(torch)  # Add extra functionalities to PyTorch for Federated Learning\n",
    "workers_ids = [\"bob\", \"alice\", \"joe\", \"anna\", \"peter\"]\n",
    "workers = []\n",
    "for worker_id in workers_ids:\n",
    "    workers.append(syft.VirtualWorker(hook, id=worker_id)) # Create a Virtual Workers\n",
    "\n",
    "# Define Learning Task Settings\n",
    "\n",
    "class TrainingArgs():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 100\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 20  # Default value for epochs\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = True\n",
    "\n",
    "args = TrainingArgs()\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()  # Use GPU if available unless CPU is specified\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "data_loader_kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Send to Workers\n",
    "In this scenario, the data is loaded and transformed from a local training dataset into a federated dataset using the `.federate` method: the dataset is split into two parts and sent to the workers Alice and Bob. This federated dataset is then given to a federated DataLoader which will iterate over remote batches. The test dataset remains unchanged as the local client will perform the test evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_loader(num_workers):\n",
    "    if num_workers > len(workers) or num_workers < 1:\n",
    "        raise ValueError(f\"Number of workers must be between 1 and {len(workers)}\")\n",
    "    if num_workers == 1:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            **data_loader_kwargs\n",
    "        )\n",
    "    train_loader = syft.FederatedDataLoader(\n",
    "        train_dataset.federate(tuple(workers[:num_workers])),  # Distribute the dataset across all workers\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            **data_loader_kwargs\n",
    "        )\n",
    "    return train_loader\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=args.test_batch_size,\n",
    "    shuffle=True,\n",
    "    **data_loader_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Definition\n",
    "- Two Convolutional Layers: 20 kernels of size 5x5 and 50 kernels of size 5x5\n",
    "- Two Fully Connected Layers with 500 and 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_workers):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(50 * 5 * 5, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        now = datetime.now()\n",
    "        timestamp_str = now.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        run_name = f\"workers_{self.num_workers}_{timestamp_str}\"\n",
    "        self.logger = WandBLogger(model=self, run_name=run_name)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = functional.relu(self.conv1(x))\n",
    "        x = functional.max_pool2d(x, 2, 2)\n",
    "        x = functional.relu(self.conv2(x))\n",
    "        x = functional.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 50*5*5)\n",
    "        x = functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return functional.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training and Testing Functions\n",
    "For training, since the data batches are distributed across Alice and Bob, the model must be sent to the correct location for each batch. Then, we perform all operations remotely with the same syntax as if we were working in a local PyTorch environment. Once finished, we retrieve the model updates and the loss to monitor improvements using the `.get()` method.\n",
    "\n",
    "For centralized training mode the `NUM_WORKERS = 1`. If greater than 1 creates federated dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(args, model, device, optimizer, epoch, num_workers):\n",
    "    assert isinstance(num_workers, int), \"num_workers should be of type int\"\n",
    "    train_loader = create_train_loader(num_workers)\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Now it is a distributed dataset\n",
    "        model.send(data.location)  # Send the model to the correct location\n",
    "        # data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = functional.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get()  # Retrieve the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get()  # Retrieve the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLocation: {}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(train_loader), loss.item(), data.location))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    loss_avg = sum(losses) / len(losses)\n",
    "    print(f\"Average train loss: {loss_avg}. Epoch: {epoch}\")\n",
    "    return loss_avg\n",
    "\n",
    "def test_model(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += functional.nll_loss(output, target, reduction='sum').item()  # Sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True)  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {} ({:.0f}%)\\n'.format(\n",
    "        test_loss, accuracy,\n",
    "        100. * accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of Weights and Biases for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msklein-at\u001b[0m (\u001b[33mspeml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/vscode/.netrc\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key=WANDB_KEY)\n",
    "class WandBLogger:\n",
    "\n",
    "    def __init__(self, enabled=True, \n",
    "                 model: torch.nn.modules=None, \n",
    "                 run_name: str=None) -> None:\n",
    "        \n",
    "        self.enabled = enabled\n",
    "\n",
    "        if self.enabled:\n",
    "            wandb.init(entity=ENTITY,\n",
    "                        project=\"federated_learning\",\n",
    "                        group=\"group_1\")\n",
    "            if run_name is None:\n",
    "                wandb.run.name = wandb.run.id    \n",
    "            else:\n",
    "                wandb.run.name = run_name  \n",
    "\n",
    "            if model is not None:\n",
    "                self.watch(model)         \n",
    "            \n",
    "    def watch(self, model, log_freq: int=1):\n",
    "        wandb.watch(model, log=\"all\", log_freq=log_freq)  \n",
    "\n",
    "    def log(self, log_dict: dict, commit=True, step=None):\n",
    "        if self.enabled:\n",
    "            if step:\n",
    "                wandb.log(log_dict, commit=commit, step=step)\n",
    "            else:\n",
    "                wandb.log(log_dict, commit=commit)\n",
    "\n",
    "    def finish(self):\n",
    "        if self.enabled:\n",
    "            wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training(num_workers):\n",
    "    model = SimpleCNN(num_workers).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train_model(args, model, device, optimizer, epoch, num_workers)\n",
    "        test_accuracy, test_loss = test_model(args, model, device, test_loader)\n",
    "\n",
    "        model.logger.log(log_dict={\n",
    "            f\"train_loss\": train_loss,\n",
    "            f\"test_loss\": test_loss,\n",
    "            f\"test_accuracy\": test_accuracy\n",
    "        }, step = epoch)\n",
    "    model.logger.finish()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), f\"cifar10_cnn_{num_workers}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/speml24_2/code/wandb/run-20240721_152506-0ndzehwp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/speml/federated_learning/runs/0ndzehwp' target=\"_blank\">smart-universe-4</a></strong> to <a href='https://wandb.ai/speml/federated_learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/speml/federated_learning' target=\"_blank\">https://wandb.ai/speml/federated_learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/speml/federated_learning/runs/0ndzehwp' target=\"_blank\">https://wandb.ai/speml/federated_learning/runs/0ndzehwp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.310346\n",
      "Train Epoch: 1 [3000/50000 (6%)]\tLoss: 2.284619\n",
      "Train Epoch: 1 [6000/50000 (12%)]\tLoss: 2.264566\n",
      "Train Epoch: 1 [9000/50000 (18%)]\tLoss: 2.204022\n",
      "Train Epoch: 1 [12000/50000 (24%)]\tLoss: 2.089017\n",
      "Train Epoch: 1 [15000/50000 (30%)]\tLoss: 2.154172\n",
      "Train Epoch: 1 [18000/50000 (36%)]\tLoss: 2.130674\n",
      "Train Epoch: 1 [21000/50000 (42%)]\tLoss: 1.982151\n",
      "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 1.911894\n",
      "Train Epoch: 1 [27000/50000 (54%)]\tLoss: 1.988635\n",
      "Train Epoch: 1 [30000/50000 (60%)]\tLoss: 1.881954\n",
      "Train Epoch: 1 [33000/50000 (66%)]\tLoss: 1.842288\n",
      "Train Epoch: 1 [36000/50000 (72%)]\tLoss: 1.956791\n",
      "Train Epoch: 1 [39000/50000 (78%)]\tLoss: 2.071869\n",
      "Train Epoch: 1 [42000/50000 (84%)]\tLoss: 1.849984\n",
      "Train Epoch: 1 [45000/50000 (90%)]\tLoss: 1.890132\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.742055\n",
      "Average train loss: 2.032656886998345. Epoch: 1\n",
      "\n",
      "Test set: Average loss: 1.7666, Accuracy: 0.3618 (36%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.740195\n",
      "Train Epoch: 2 [3000/50000 (6%)]\tLoss: 1.749783\n",
      "Train Epoch: 2 [6000/50000 (12%)]\tLoss: 1.829816\n",
      "Train Epoch: 2 [9000/50000 (18%)]\tLoss: 1.896803\n",
      "Train Epoch: 2 [12000/50000 (24%)]\tLoss: 1.617020\n",
      "Train Epoch: 2 [15000/50000 (30%)]\tLoss: 1.584560\n",
      "Train Epoch: 2 [18000/50000 (36%)]\tLoss: 1.796532\n",
      "Train Epoch: 2 [21000/50000 (42%)]\tLoss: 1.651888\n",
      "Train Epoch: 2 [24000/50000 (48%)]\tLoss: 1.479593\n",
      "Train Epoch: 2 [27000/50000 (54%)]\tLoss: 1.772129\n",
      "Train Epoch: 2 [30000/50000 (60%)]\tLoss: 1.527369\n",
      "Train Epoch: 2 [33000/50000 (66%)]\tLoss: 1.647952\n",
      "Train Epoch: 2 [36000/50000 (72%)]\tLoss: 1.608476\n",
      "Train Epoch: 2 [39000/50000 (78%)]\tLoss: 1.608889\n",
      "Train Epoch: 2 [42000/50000 (84%)]\tLoss: 1.496896\n",
      "Train Epoch: 2 [45000/50000 (90%)]\tLoss: 1.535884\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.387550\n",
      "Average train loss: 1.6430196902331184. Epoch: 2\n",
      "\n",
      "Test set: Average loss: 1.4997, Accuracy: 0.4618 (46%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.590496\n",
      "Train Epoch: 3 [3000/50000 (6%)]\tLoss: 1.481304\n",
      "Train Epoch: 3 [6000/50000 (12%)]\tLoss: 1.433250\n",
      "Train Epoch: 3 [9000/50000 (18%)]\tLoss: 1.365172\n",
      "Train Epoch: 3 [12000/50000 (24%)]\tLoss: 1.450006\n",
      "Train Epoch: 3 [15000/50000 (30%)]\tLoss: 1.343434\n",
      "Train Epoch: 3 [18000/50000 (36%)]\tLoss: 1.457972\n",
      "Train Epoch: 3 [21000/50000 (42%)]\tLoss: 1.574115\n",
      "Train Epoch: 3 [24000/50000 (48%)]\tLoss: 1.534524\n",
      "Train Epoch: 3 [27000/50000 (54%)]\tLoss: 1.387889\n",
      "Train Epoch: 3 [30000/50000 (60%)]\tLoss: 1.436730\n"
     ]
    }
   ],
   "source": [
    "start_training(num_workers=NUM_WORKERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speml_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
