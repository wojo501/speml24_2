{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Pytorch\n",
    "First, import the Pytorch dependencies.\n",
    "\n",
    "Then we define the remote workers Alice and Bob, who will host the remote data while a local worker (or client) will manage the learning task. Note that we use virtual workers: these workers act just like normal remote workers except that they exist within the same Python program. Thus, we still serialize the commands to be exchanged between the workers but we donâ€™t actually send them over the network. This approach allows us to avoid network-related issues and focus on evaluation of the approach on a local setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import syft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = syft.TorchHook(torch)  # Add extra functionalities to PyTorch for Federated Learning\n",
    "bob_worker = syft.VirtualWorker(hook, id=\"bob\")  # Define remote worker Bob\n",
    "alice_worker = syft.VirtualWorker(hook, id=\"alice\")\n",
    "\n",
    "# Define Learning Task Settings\n",
    "\n",
    "class TrainingArgs():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 100\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 10  # Default value for epochs\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = False\n",
    "\n",
    "args = TrainingArgs()\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()  # Use GPU if available unless CPU is specified\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "data_loader_kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Send to Workers\n",
    "In this scenario, the data is loaded and transformed from a local training dataset into a federated dataset using the `.federate` method: the dataset is split into two parts and sent to the workers Alice and Bob. This federated dataset is then given to a federated DataLoader which will iterate over remote batches. The test dataset remains unchanged as the local client will perform the test evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "federated_train_loader = syft.FederatedDataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        \"../data\", train=True, download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]\n",
    "        ),\n",
    "    ).federate((bob_worker, alice_worker)),  # Distribute the dataset across all workers\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    **data_loader_kwargs\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        \"../data\", train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=args.test_batch_size,\n",
    "    shuffle=True,\n",
    "    **data_loader_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Definition\n",
    "- Two Convolutional Layers: 20 kernels of size 5x5 and 50 kernels of size 5x5\n",
    "- Two Fully Connected Layers with 500 and 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(50 * 5 * 5, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = functional.relu(self.conv1(x))\n",
    "        x = functional.max_pool2d(x, 2, 2)\n",
    "        x = functional.relu(self.conv2(x))\n",
    "        x = functional.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 50*5*5)\n",
    "        x = functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return functional.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training and Testing Functions\n",
    "For training, since the data batches are distributed across Alice and Bob, the model must be sent to the correct location for each batch. Then, we perform all operations remotely with the same syntax as if we were working in a local PyTorch environment. Once finished, we retrieve the model updates and the loss to monitor improvements using the `.get()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader):  # Now it is a distributed dataset\n",
    "        model.send(data.location)  # Send the model to the correct location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = functional.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get()  # Retrieve the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get()  # Retrieve the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test_model(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += functional.nll_loss(output, target, reduction='sum').item()  # Sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True)  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.300576\n",
      "Train Epoch: 1 [3000/50000 (6%)]\tLoss: 2.291166\n",
      "Train Epoch: 1 [6000/50000 (12%)]\tLoss: 2.276882\n",
      "Train Epoch: 1 [9000/50000 (18%)]\tLoss: 2.219726\n",
      "Train Epoch: 1 [12000/50000 (24%)]\tLoss: 2.206737\n",
      "Train Epoch: 1 [15000/50000 (30%)]\tLoss: 2.124860\n",
      "Train Epoch: 1 [18000/50000 (36%)]\tLoss: 2.037491\n",
      "Train Epoch: 1 [21000/50000 (42%)]\tLoss: 1.972625\n",
      "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 1.956890\n",
      "Train Epoch: 1 [27000/50000 (54%)]\tLoss: 1.941134\n",
      "Train Epoch: 1 [30000/50000 (60%)]\tLoss: 1.781886\n",
      "Train Epoch: 1 [33000/50000 (66%)]\tLoss: 1.881317\n",
      "Train Epoch: 1 [36000/50000 (72%)]\tLoss: 1.930419\n",
      "Train Epoch: 1 [39000/50000 (78%)]\tLoss: 1.764993\n",
      "Train Epoch: 1 [42000/50000 (84%)]\tLoss: 1.713211\n",
      "Train Epoch: 1 [45000/50000 (90%)]\tLoss: 1.708090\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.824077\n",
      "\n",
      "Test set: Average loss: 1.7678, Accuracy: 3726/10000 (37%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.732595\n",
      "Train Epoch: 2 [3000/50000 (6%)]\tLoss: 1.852025\n",
      "Train Epoch: 2 [6000/50000 (12%)]\tLoss: 1.644139\n",
      "Train Epoch: 2 [9000/50000 (18%)]\tLoss: 1.687958\n",
      "Train Epoch: 2 [12000/50000 (24%)]\tLoss: 1.700859\n",
      "Train Epoch: 2 [15000/50000 (30%)]\tLoss: 1.697796\n",
      "Train Epoch: 2 [18000/50000 (36%)]\tLoss: 1.537764\n",
      "Train Epoch: 2 [21000/50000 (42%)]\tLoss: 1.610475\n",
      "Train Epoch: 2 [24000/50000 (48%)]\tLoss: 1.677738\n",
      "Train Epoch: 2 [27000/50000 (54%)]\tLoss: 1.668839\n",
      "Train Epoch: 2 [30000/50000 (60%)]\tLoss: 1.627023\n",
      "Train Epoch: 2 [33000/50000 (66%)]\tLoss: 1.699413\n",
      "Train Epoch: 2 [36000/50000 (72%)]\tLoss: 1.554176\n",
      "Train Epoch: 2 [39000/50000 (78%)]\tLoss: 1.468811\n",
      "Train Epoch: 2 [42000/50000 (84%)]\tLoss: 1.527920\n",
      "Train Epoch: 2 [45000/50000 (90%)]\tLoss: 1.556315\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.646897\n",
      "\n",
      "Test set: Average loss: 1.5156, Accuracy: 4589/10000 (46%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.557589\n",
      "Train Epoch: 3 [3000/50000 (6%)]\tLoss: 1.443731\n",
      "Train Epoch: 3 [6000/50000 (12%)]\tLoss: 1.465356\n",
      "Train Epoch: 3 [9000/50000 (18%)]\tLoss: 1.738116\n",
      "Train Epoch: 3 [12000/50000 (24%)]\tLoss: 1.417171\n",
      "Train Epoch: 3 [15000/50000 (30%)]\tLoss: 1.639794\n",
      "Train Epoch: 3 [18000/50000 (36%)]\tLoss: 1.411347\n",
      "Train Epoch: 3 [21000/50000 (42%)]\tLoss: 1.495058\n",
      "Train Epoch: 3 [24000/50000 (48%)]\tLoss: 1.453908\n",
      "Train Epoch: 3 [27000/50000 (54%)]\tLoss: 1.383649\n",
      "Train Epoch: 3 [30000/50000 (60%)]\tLoss: 1.461952\n",
      "Train Epoch: 3 [33000/50000 (66%)]\tLoss: 1.456034\n",
      "Train Epoch: 3 [36000/50000 (72%)]\tLoss: 1.456985\n",
      "Train Epoch: 3 [39000/50000 (78%)]\tLoss: 1.500111\n",
      "Train Epoch: 3 [42000/50000 (84%)]\tLoss: 1.415628\n",
      "Train Epoch: 3 [45000/50000 (90%)]\tLoss: 1.448828\n",
      "Train Epoch: 3 [48000/50000 (96%)]\tLoss: 1.443608\n",
      "\n",
      "Test set: Average loss: 1.3933, Accuracy: 5105/10000 (51%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.354127\n",
      "Train Epoch: 4 [3000/50000 (6%)]\tLoss: 1.337424\n",
      "Train Epoch: 4 [6000/50000 (12%)]\tLoss: 1.486003\n",
      "Train Epoch: 4 [9000/50000 (18%)]\tLoss: 1.368594\n",
      "Train Epoch: 4 [12000/50000 (24%)]\tLoss: 1.538029\n",
      "Train Epoch: 4 [15000/50000 (30%)]\tLoss: 1.448976\n",
      "Train Epoch: 4 [18000/50000 (36%)]\tLoss: 1.397041\n",
      "Train Epoch: 4 [21000/50000 (42%)]\tLoss: 1.376214\n",
      "Train Epoch: 4 [24000/50000 (48%)]\tLoss: 1.369925\n",
      "Train Epoch: 4 [27000/50000 (54%)]\tLoss: 1.343873\n",
      "Train Epoch: 4 [30000/50000 (60%)]\tLoss: 1.380116\n",
      "Train Epoch: 4 [33000/50000 (66%)]\tLoss: 1.509282\n",
      "Train Epoch: 4 [36000/50000 (72%)]\tLoss: 1.411857\n",
      "Train Epoch: 4 [39000/50000 (78%)]\tLoss: 1.222865\n",
      "Train Epoch: 4 [42000/50000 (84%)]\tLoss: 1.372934\n",
      "Train Epoch: 4 [45000/50000 (90%)]\tLoss: 1.414660\n",
      "Train Epoch: 4 [48000/50000 (96%)]\tLoss: 1.267325\n",
      "\n",
      "Test set: Average loss: 1.3229, Accuracy: 5335/10000 (53%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.357239\n",
      "Train Epoch: 5 [3000/50000 (6%)]\tLoss: 1.369426\n",
      "Train Epoch: 5 [6000/50000 (12%)]\tLoss: 1.433004\n",
      "Train Epoch: 5 [9000/50000 (18%)]\tLoss: 1.271656\n",
      "Train Epoch: 5 [12000/50000 (24%)]\tLoss: 1.367314\n",
      "Train Epoch: 5 [15000/50000 (30%)]\tLoss: 1.194985\n",
      "Train Epoch: 5 [18000/50000 (36%)]\tLoss: 1.069528\n",
      "Train Epoch: 5 [21000/50000 (42%)]\tLoss: 1.493422\n",
      "Train Epoch: 5 [24000/50000 (48%)]\tLoss: 1.254798\n",
      "Train Epoch: 5 [27000/50000 (54%)]\tLoss: 1.496857\n",
      "Train Epoch: 5 [30000/50000 (60%)]\tLoss: 1.213941\n",
      "Train Epoch: 5 [33000/50000 (66%)]\tLoss: 1.321085\n",
      "Train Epoch: 5 [36000/50000 (72%)]\tLoss: 1.412134\n",
      "Train Epoch: 5 [39000/50000 (78%)]\tLoss: 1.301855\n",
      "Train Epoch: 5 [42000/50000 (84%)]\tLoss: 1.322137\n",
      "Train Epoch: 5 [45000/50000 (90%)]\tLoss: 1.343883\n",
      "Train Epoch: 5 [48000/50000 (96%)]\tLoss: 1.295000\n",
      "\n",
      "Test set: Average loss: 1.2527, Accuracy: 5528/10000 (55%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.276700\n",
      "Train Epoch: 6 [3000/50000 (6%)]\tLoss: 1.179108\n",
      "Train Epoch: 6 [6000/50000 (12%)]\tLoss: 1.350108\n",
      "Train Epoch: 6 [9000/50000 (18%)]\tLoss: 1.288170\n",
      "Train Epoch: 6 [12000/50000 (24%)]\tLoss: 1.275165\n",
      "Train Epoch: 6 [15000/50000 (30%)]\tLoss: 1.143316\n",
      "Train Epoch: 6 [18000/50000 (36%)]\tLoss: 1.145468\n",
      "Train Epoch: 6 [21000/50000 (42%)]\tLoss: 1.426272\n",
      "Train Epoch: 6 [24000/50000 (48%)]\tLoss: 1.109712\n",
      "Train Epoch: 6 [27000/50000 (54%)]\tLoss: 1.079011\n",
      "Train Epoch: 6 [30000/50000 (60%)]\tLoss: 1.189044\n",
      "Train Epoch: 6 [33000/50000 (66%)]\tLoss: 1.185262\n",
      "Train Epoch: 6 [36000/50000 (72%)]\tLoss: 1.162089\n",
      "Train Epoch: 6 [39000/50000 (78%)]\tLoss: 1.103020\n",
      "Train Epoch: 6 [42000/50000 (84%)]\tLoss: 1.023447\n",
      "Train Epoch: 6 [45000/50000 (90%)]\tLoss: 1.155688\n",
      "Train Epoch: 6 [48000/50000 (96%)]\tLoss: 1.067442\n",
      "\n",
      "Test set: Average loss: 1.2011, Accuracy: 5698/10000 (57%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.018252\n",
      "Train Epoch: 7 [3000/50000 (6%)]\tLoss: 1.029192\n",
      "Train Epoch: 7 [6000/50000 (12%)]\tLoss: 1.135337\n",
      "Train Epoch: 7 [9000/50000 (18%)]\tLoss: 1.124055\n",
      "Train Epoch: 7 [12000/50000 (24%)]\tLoss: 1.197727\n",
      "Train Epoch: 7 [15000/50000 (30%)]\tLoss: 1.455610\n",
      "Train Epoch: 7 [18000/50000 (36%)]\tLoss: 1.144410\n",
      "Train Epoch: 7 [21000/50000 (42%)]\tLoss: 1.278988\n",
      "Train Epoch: 7 [24000/50000 (48%)]\tLoss: 0.985433\n",
      "Train Epoch: 7 [27000/50000 (54%)]\tLoss: 1.438789\n",
      "Train Epoch: 7 [30000/50000 (60%)]\tLoss: 1.272237\n",
      "Train Epoch: 7 [33000/50000 (66%)]\tLoss: 1.059842\n",
      "Train Epoch: 7 [36000/50000 (72%)]\tLoss: 1.227594\n",
      "Train Epoch: 7 [39000/50000 (78%)]\tLoss: 1.398742\n",
      "Train Epoch: 7 [42000/50000 (84%)]\tLoss: 1.009782\n",
      "Train Epoch: 7 [45000/50000 (90%)]\tLoss: 0.981355\n",
      "Train Epoch: 7 [48000/50000 (96%)]\tLoss: 1.184066\n",
      "\n",
      "Test set: Average loss: 1.1600, Accuracy: 5908/10000 (59%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.983040\n",
      "Train Epoch: 8 [3000/50000 (6%)]\tLoss: 1.140597\n",
      "Train Epoch: 8 [6000/50000 (12%)]\tLoss: 1.079944\n",
      "Train Epoch: 8 [9000/50000 (18%)]\tLoss: 1.037575\n",
      "Train Epoch: 8 [12000/50000 (24%)]\tLoss: 1.126651\n",
      "Train Epoch: 8 [15000/50000 (30%)]\tLoss: 1.103304\n",
      "Train Epoch: 8 [18000/50000 (36%)]\tLoss: 1.134739\n",
      "Train Epoch: 8 [21000/50000 (42%)]\tLoss: 0.988123\n",
      "Train Epoch: 8 [24000/50000 (48%)]\tLoss: 1.035603\n",
      "Train Epoch: 8 [27000/50000 (54%)]\tLoss: 1.267654\n",
      "Train Epoch: 8 [30000/50000 (60%)]\tLoss: 1.073501\n",
      "Train Epoch: 8 [33000/50000 (66%)]\tLoss: 1.007300\n",
      "Train Epoch: 8 [36000/50000 (72%)]\tLoss: 1.010062\n",
      "Train Epoch: 8 [39000/50000 (78%)]\tLoss: 1.051293\n",
      "Train Epoch: 8 [42000/50000 (84%)]\tLoss: 1.112947\n",
      "Train Epoch: 8 [45000/50000 (90%)]\tLoss: 1.063475\n",
      "Train Epoch: 8 [48000/50000 (96%)]\tLoss: 1.377404\n",
      "\n",
      "Test set: Average loss: 1.1285, Accuracy: 6010/10000 (60%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.105814\n",
      "Train Epoch: 9 [3000/50000 (6%)]\tLoss: 1.248434\n",
      "Train Epoch: 9 [6000/50000 (12%)]\tLoss: 0.916659\n",
      "Train Epoch: 9 [9000/50000 (18%)]\tLoss: 1.151976\n",
      "Train Epoch: 9 [12000/50000 (24%)]\tLoss: 1.186507\n",
      "Train Epoch: 9 [15000/50000 (30%)]\tLoss: 1.032151\n",
      "Train Epoch: 9 [18000/50000 (36%)]\tLoss: 0.879076\n",
      "Train Epoch: 9 [21000/50000 (42%)]\tLoss: 1.018437\n",
      "Train Epoch: 9 [24000/50000 (48%)]\tLoss: 0.996577\n",
      "Train Epoch: 9 [27000/50000 (54%)]\tLoss: 0.955147\n",
      "Train Epoch: 9 [30000/50000 (60%)]\tLoss: 1.068063\n",
      "Train Epoch: 9 [33000/50000 (66%)]\tLoss: 0.981654\n",
      "Train Epoch: 9 [36000/50000 (72%)]\tLoss: 1.021631\n",
      "Train Epoch: 9 [39000/50000 (78%)]\tLoss: 1.107282\n",
      "Train Epoch: 9 [42000/50000 (84%)]\tLoss: 0.808426\n",
      "Train Epoch: 9 [45000/50000 (90%)]\tLoss: 1.029127\n",
      "Train Epoch: 9 [48000/50000 (96%)]\tLoss: 1.205248\n",
      "\n",
      "Test set: Average loss: 1.1269, Accuracy: 6031/10000 (60%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.152606\n",
      "Train Epoch: 10 [3000/50000 (6%)]\tLoss: 1.109324\n",
      "Train Epoch: 10 [6000/50000 (12%)]\tLoss: 0.969471\n",
      "Train Epoch: 10 [9000/50000 (18%)]\tLoss: 1.263267\n",
      "Train Epoch: 10 [12000/50000 (24%)]\tLoss: 0.921281\n",
      "Train Epoch: 10 [15000/50000 (30%)]\tLoss: 0.885066\n",
      "Train Epoch: 10 [18000/50000 (36%)]\tLoss: 0.898784\n",
      "Train Epoch: 10 [21000/50000 (42%)]\tLoss: 0.973815\n",
      "Train Epoch: 10 [24000/50000 (48%)]\tLoss: 1.215642\n",
      "Train Epoch: 10 [27000/50000 (54%)]\tLoss: 1.014032\n",
      "Train Epoch: 10 [30000/50000 (60%)]\tLoss: 1.075674\n",
      "Train Epoch: 10 [33000/50000 (66%)]\tLoss: 0.924891\n",
      "Train Epoch: 10 [36000/50000 (72%)]\tLoss: 1.008451\n",
      "Train Epoch: 10 [39000/50000 (78%)]\tLoss: 1.193395\n",
      "Train Epoch: 10 [42000/50000 (84%)]\tLoss: 1.213576\n",
      "Train Epoch: 10 [45000/50000 (90%)]\tLoss: 0.897757\n",
      "Train Epoch: 10 [48000/50000 (96%)]\tLoss: 0.960839\n",
      "\n",
      "Test set: Average loss: 1.1046, Accuracy: 6094/10000 (61%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train_model(args, model, device, federated_train_loader, optimizer, epoch)\n",
    "    test_model(args, model, device, test_loader)\n",
    "\n",
    "if args.save_model:\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speml_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
